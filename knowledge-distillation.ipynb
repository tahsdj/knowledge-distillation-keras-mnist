{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers import Dense,GlobalAveragePooling2D, MaxPool2D\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from keras.utils import to_categorical # for encoding data to one not form\n",
    "import tensorflow as tf\n",
    "from keras.objectives import categorical_crossentropy\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# download mnist\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build teacher CNN\n",
    "def create_teacher_CNN():\n",
    "    inputs = Input(shape=(28, 28, 1))\n",
    "\n",
    "    x = Conv2D(32,(3,3),padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = MaxPool2D(pool_size=(3,3),strides=(2,2),padding='same')(x)\n",
    "\n",
    "    x = Conv2D(64,(3,3),padding='same', kernel_initializer='he_normal')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = MaxPool2D(pool_size=(3,3),strides=(2,2),padding='same')(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "\n",
    "    x = Dense(512, activation=\"relu\")(x)\n",
    "    x = Dense(10, name=\"logits\")(x)\n",
    "    predictions = Activation('softmax')(x)\n",
    "\n",
    "    return Model(inputs=inputs, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build student net\n",
    "def build_student_net():\n",
    "    inputs = Input(shape=(28,28,1))\n",
    "    x = Flatten()(inputs)\n",
    "    x = Dense(1024,activation=\"relu\")(x) \n",
    "    logits = Dense(10)(x)\n",
    "    return Model(inputs=inputs, outputs=[logits, logits])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teacher_model = create_teacher_CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teacher_lr = 1e-3\n",
    "opt = Adam(lr=teacher_lr)\n",
    "\n",
    "teacher_model.compile(\n",
    "    loss=['categorical_crossentropy'],\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train teacher model\n",
    "iteration = 8000\n",
    "for i in range(iteration):\n",
    "  \n",
    "    # set learning rate schedule\n",
    "    if i > iteration*0.4: \n",
    "        K.set_value(teacher_model.optimizer.lr, teacher_lr*0.2)\n",
    "    if i > iteration*0.7: \n",
    "        K.set_value(teacher_model.optimizer.lr, teacher_lr*0.02)\n",
    "    if i > iteration*0.9:\n",
    "        K.set_value(teacher_model.optimizer.lr, teacher_lr*0.002)\n",
    "\n",
    "    batch_x, batch_y = mnist.train.next_batch(128)\n",
    "    batch_x = batch_x.reshape((-1,28,28,1))\n",
    "  \n",
    "    tr_results = teacher_model.train_on_batch(batch_x, batch_y)\n",
    "    tr_loss, tr_acc = tr_results[0], tr_results[1]\n",
    "  \n",
    "    if i % 100 == 0:\n",
    "    \n",
    "        test_x, test_y = mnist.test.images, mnist.test.labels\n",
    "        test_x = test_x.reshape((-1,28,28,1))\n",
    "        val_results = teacher_model.test_on_batch(test_x, test_y)\n",
    "        val_loss, val_acc = val_results[0], val_results[1]\n",
    "        print('Iteration: {:}  tr_loss; {:.6}  tr_acc: {:.4}  val_loss: {:.6}  val_acc: {:.4}'.format(i, tr_loss, tr_acc, val_loss, val_acc))\n",
    "    \n",
    "test_x, test_y = mnist.test.images, mnist.test.labels\n",
    "test_x = test_x.reshape((-1,28,28,1))\n",
    "val_results = teacher_model.test_on_batch(test_x, test_y)\n",
    "val_loss, val_acc = val_results[0], val_results[1]\n",
    "print('============== training done =========================')\n",
    "print('val_loss: {:.6}  val_acc: {:.4}'.format(val_loss, val_acc))\n",
    "print('======================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save teacher model\n",
    "teacher_model.save('./teacher_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_teacher_logits(teacher_net):\n",
    "    logits_layer = teacher_net.get_layer('logits').output\n",
    "    return Model(inputs=teacher_net.input,outputs=logits_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load teacher pretrained model\n",
    "teacher_model = load_model('./teacher_model.h5')\n",
    "\n",
    "# build student net\n",
    "student = build_student_net()\n",
    "teacher_kd_model = get_teacher_logits(teacher_model)\n",
    "teacher_kd_model.trainable = False\n",
    "\n",
    "\n",
    "T = 7 # temperature\n",
    "a = 0.9 # ratio of kd\n",
    "\n",
    "def kd_loss(y_true, y_pred):\n",
    "    soft_y = K.softmax(y_true/T)\n",
    "    soft_pred = K.softmax(y_pred/T)\n",
    "    return categorical_crossentropy(soft_y, soft_pred)\n",
    "\n",
    "\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    y_pred = K.softmax(y_pred)\n",
    "    return categorical_crossentropy(y_true, y_pred)\n",
    "\n",
    "\n",
    "student_lr = 1e-3\n",
    "opt = Adam(lr=student_lr)\n",
    "student.compile(\n",
    "    loss=[kd_loss, cross_entropy],\n",
    "    optimizer=opt,\n",
    "    metrics=['accuracy'],\n",
    "    loss_weights=[a,(1.0-a)] # a is the ratio for kd, (1-a) is for ground truth\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train student model\n",
    "counter = 0\n",
    "iteration = 8000\n",
    "for i in range(iteration):\n",
    "  \n",
    "    # learning rate schedule\n",
    "    if i > iteration*0.4: \n",
    "        K.set_value(student.optimizer.lr, student_lr*0.2)\n",
    "    if i > iteration*0.7: \n",
    "        K.set_value(student.optimizer.lr, student_lr*0.02)\n",
    "    if i > iteration*0.9:\n",
    "        K.set_value(student.optimizer.lr, student_lr*0.002)\n",
    "  \n",
    "    # get ground truth\n",
    "    batch_x, batch_y = mnist.train.next_batch(128)\n",
    "    batch_x = batch_x.reshape((-1,28,28,1))\n",
    "\n",
    "    # get teacher label\n",
    "    teacher_y = teacher_kd_model.predict(batch_x)\n",
    "\n",
    "    tr_results = student.train_on_batch(batch_x, [teacher_y, batch_y])\n",
    "    tr_loss, tr_acc = tr_results[0], tr_results[-1]\n",
    "  \n",
    "    if i % 100 == 0:\n",
    "        test_x, test_y = mnist.test.images, mnist.test.labels\n",
    "        test_x = test_x.reshape((-1,28,28,1))\n",
    "\n",
    "        teacher_y = teacher_kd_model.predict(test_x)\n",
    "\n",
    "        val_results = student.test_on_batch(test_x, [teacher_y, test_y])\n",
    "        val_loss, val_acc = val_results[0], val_results[-1]\n",
    "\n",
    "        print('Iteration: {:}  tr_loss; {:.6}  tr_acc: {:.4}  val_loss: {:.6}  val_acc: {:.4}'.format(i, tr_loss, tr_acc, val_loss, val_acc))\n",
    "\n",
    "test_x, test_y = mnist.test.images, mnist.test.labels\n",
    "test_x = test_x.reshape((-1,28,28,1))\n",
    "teacher_y = teacher_kd_model.predict(test_x)\n",
    "val_results = student.test_on_batch(test_x, [teacher_y, test_y])\n",
    "val_loss, val_acc = val_results[0], val_results[-1]\n",
    "print('============== training done =========================')\n",
    "print('val_loss: {:.6}  val_acc: {:.4}'.format(val_loss, val_acc))\n",
    "print('======================================================')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
